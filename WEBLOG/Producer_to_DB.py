from pyspark.sql import SparkSessionfrom pyspark.sql.functions import col, split, to_timestampfrom pyspark.sql import functions as F# Initialize SparkSessionspark = SparkSession.builder.appName("Kafka_Spark").config("spark.sql.streaming.checkpointLocation", "/tmp/spark_checkpoints").config("spark.sql.adaptive.enabled", "false").getOrCreate()# Read Kafka Streamdf = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe", "real_time_weblogs3").load()df1 = df.selectExpr("CAST(value AS STRING)")# Split the value and extract columnsdf2 = df1.withColumn("value_split", F.split(F.col("value"), ",")).withColumn("log_timestamp", F.col("value_split").getItem(0)).withColumn("ip_address", F.col("value_split").getItem(1)).withColumn("host_name", F.col("value_split").getItem(2)).withColumn("url", F.col("value_split").getItem(3)).withColumn("response_code", F.col("value_split").getItem(4).cast("string")).drop("value_split", "value")# Write Stream to Console (optional for debugging)df2.writeStream.format("console").option("truncate", "false").outputMode("append").start()# Write Stream to PostgreSQLdf2.writeStream.foreachBatch(lambda batch_df, batch_id: (    batch_df.write \    .format("jdbc") \    .option("url", "jdbc:postgresql://localhost:5432/mydatabase") \    .option("dbtable", "weblogdetails1") \    .option("user", "hudi_user") \    .option("password", "password") \    .option("driver", "org.postgresql.Driver") \    .mode("append")  # Ensure this is append mode    .save())).outputMode("append").start().awaitTermination()