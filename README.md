This project demonstrates an end-to-end data workflow that ingests logs from a Kafka topic, processes them using PySpark and Spark Structured Streaming, and stores the results in Hive and MySQL databases. Using Sqoop, we incrementally migrate data between systems.
![Data_Pipeline_Architecture](https://github.com/user-attachments/assets/c5c6fe9e-e3bc-4e6c-9b3f-33d434ab9f88)

