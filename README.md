This project demonstrates an end-to-end data workflow that ingests logs from a Kafka topic, processes them using PySpark and Spark Structured Streaming, and stores the results in Hive and MySQL databases. Using Sqoop, we incrementally migrate data between systems.

![Data_pipeline_Architecture](https://github.com/user-attachments/assets/d68dd371-336b-41ba-b926-341821f7cd45)

